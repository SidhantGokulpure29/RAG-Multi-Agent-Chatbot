# -*- coding: utf-8 -*-
"""RAG and Multi Agent Workflow with LLM-as-Judge

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SIRi4V8G8JjQGiSwkPWwmq__THkxfe7c

## **Importing Libraries**
"""

!pip install groq langgraph langchain_community chromadb sentence-transformers langchain-text-splitters

from google.colab import userdata
from groq import Groq
from typing import Annotated, Literal
from typing_extensions import TypedDict

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

"""## **Initializing Groq client**"""

GROQ_API_KEY = userdata.get('groqApiKey')
client = Groq(api_key=GROQ_API_KEY)

SOURCE_URL = "https://joinseven.medium.com/blog-series-genai-a-brief-introduction-in-generative-ai-4e11154df3f2"
loader = WebBaseLoader(web_paths=[SOURCE_URL])
docs = loader.load()
print(docs)

"""## **Splitting document into chunk**"""

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = splitter.split_documents(docs)

"""## **Storing in Chromadb**"""

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(split_docs, embeddings)

"""## **Retriever**"""

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

"""## **State Definition**"""

class MessageClassifier(BaseModel):
  message_type: Literal["rag","logical"] = Field(
  ...,
  description="Classify if the message requires retrieval (rag) or a direct logical response."
  )

# class State(TypedDict):
#   messages: Annotated[list, add_messages]
#   message_type: str | None

class State(TypedDict):
    messages: Annotated[list, add_messages]
    message_type: str | None
    evaluation: dict | None

"""## **Classifier Node**"""

def classify_messages(state: State):
    last_message = state["messages"][-1]
    resp = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {
                "role": "system",
                "content": (
                    "Classify the user message as 'rag' if it likely benefits from external context "
                    "(e.g., asks about topics in the scraped document or requests factual info). "
                    "Otherwise classify as 'logical'. Reply with only 'rag' or 'logical'."
                )
            },
            {"role": "user", "content": last_message.content}
        ],
        temperature=0
    )
    label = resp.choices[0].message.content.strip().lower()
    if "rag" in label:
        return {"message_type": "rag"}
    elif "logical" in label:
        return {"message_type": "logical"}
    else:
        return {"message_type": "logical"}

"""## **RAG Agent**"""

def rag_agent(state: State):
  last_message = state["messages"][-1]
  retreived_docs = retriever.invoke(last_message.content)
  context = "\n\n".join([doc.page_content for doc in retrieved_docs[:3]]) or "No relevant context found."

  resp = client.chat.completions.create(
      model="llama-3.1-8b-instant",
      messages=[
          {
              "role":"system",
              "content":(
                  "You are a helpful assistant. Use the provided context to answer the user's question. "
                  "If context is insufficient, say so briefly and respond with best-effort clarity."
              )
          },
          {
              "role":"user",
              "content":f"Question: {last_message.content}\n\nContext:\n{context}"
          }
      ],
      temperature=0
  )

  reply = resp.choices[0].message.content.strip()
  return {"messages": [{"role": "assistant", "content": reply}]}

"""## **Logical Agent**"""

def logical_agent(state: State):
  last_message = state["messages"][-1]

  resp = client.chat.completions.create(
      model="llama-3.1-8b-instant",
      messages=[
          {
              "role":"system",
              "content":"You are a concise, factual assistant. Answer clearly and directly."

          },
          {
              "role":"user",
              "content":last_message.content
          }
      ],
      temperature=0
  )

  reply = resp.choices[0].message.content.strip()
  return {"messages": [{"role": "assistant", "content": reply}]}

import json
from langchain_core.messages import AIMessage, HumanMessage

def evaluator_agent(state: State):
    """
    Evaluates the assistant's last response against the user's query.
    Compatible with LangGraph message objects.
    """

    user_query = None
    assistant_answer = None

    # Walk backwards to find last assistant + user message
    for msg in reversed(state["messages"]):
        if isinstance(msg, AIMessage) and assistant_answer is None:
            assistant_answer = msg.content
        elif isinstance(msg, HumanMessage):
            user_query = msg.content
            break

    mode = state.get("message_type", "logical")

    prompt = f"""
You are an expert LLM evaluator.

Evaluate the assistant answer for the given user query.

Give scores from 0 to 10 for each metric.

Metrics:
1. Relevance â€“ Does the answer address the question?
2. Correctness â€“ Is the information accurate?
3. Completeness â€“ Does the answer fully cover all aspects of the query?
4. Hallucination Risk â€“ Likelihood of made-up or unsupported claims

User Query:
{user_query}

Assistant Answer:
{assistant_answer}

Mode:
{mode}

Return ONLY valid JSON in this format:
{{
  "relevance": number,
  "correctness": number,
  "completeness": number,
  "hallucination_risk": number,
}}
"""

    resp = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    try:
        evaluation = json.loads(resp.choices[0].message.content)
    except Exception:
        evaluation = {"error": "Evaluation parsing failed"}

    return {"evaluation": evaluation}

"""## **Graph Builder with LangGraph**"""

graph_builder = StateGraph(State)

graph_builder.add_node("classifier", classify_messages)
graph_builder.add_node("rag_agent", rag_agent)
graph_builder.add_node("logical_agent", logical_agent)

graph_builder.add_node("evaluator", evaluator_agent)

graph_builder.add_edge(START, "classifier")

graph_builder.add_conditional_edges(
    "classifier",
    lambda state: state.get("message_type"),
    {
        "rag": "rag_agent",
        "logical": "logical_agent"
    }
)

# graph_builder.add_edge("rag_agent", END)
# graph_builder.add_edge("logical_agent", END)

graph_builder.add_edge("rag_agent", "evaluator")
graph_builder.add_edge("logical_agent", "evaluator")
graph_builder.add_edge("evaluator", END)


graph = graph_builder.compile()

def run_chatbot():
    state = {"messages": [], "message_type": None}
    print("Type 'exit' to quit.")
    while True:
        user_input = input("Message: ")
        if user_input.strip().lower() == "exit":
            print("Bye")
            break

        # Add user message to state
        state["messages"].append({"role": "user", "content": user_input})

        # Invoke your LangGraph workflow (multi-agent pipeline)
        state = graph.invoke(state)

        # Find and print the assistant's reply
        assistant_output = None
        for msg in state["messages"]:
            if isinstance(msg, dict):
                if msg.get("role") == "assistant":
                    assistant_output = msg.get("content")
            else:
                # Handle LangChain message objects if they appear
                role = getattr(msg, "role", None) or getattr(msg, "type", None)
                content = getattr(msg, "content", None)
                if role in ["assistant", "ai"]:
                    assistant_output = content

        if assistant_output:
            print(f"\nAssistant: {assistant_output}\n")


            if state.get("evaluation"):
              print("ðŸ“Š Evaluation Metrics:")
              for k, v in state["evaluation"].items():
                print(f"  {k}: {v}")
            print("-----------------------------------------------------------------------------------------------")
        else:
            print("\nNo assistant output found.\n")

run_chatbot()