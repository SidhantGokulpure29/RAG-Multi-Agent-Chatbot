# -*- coding: utf-8 -*-
"""RAG and Multi Agent Workflow

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SIRi4V8G8JjQGiSwkPWwmq__THkxfe7c

## **Importing Libraries**
"""

!pip install groq langgraph langchain_community chromadb sentence-transformers langchain-text-splitters deepeval

from google.colab import userdata
from groq import Groq
from typing import Annotated, Literal
from typing_extensions import TypedDict

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

"""## **Initializing Groq client**"""

GROQ_API_KEY = userdata.get('groqApiKey')
client = Groq(api_key=GROQ_API_KEY)

SOURCE_URL = "https://joinseven.medium.com/blog-series-genai-a-brief-introduction-in-generative-ai-4e11154df3f2"
loader = WebBaseLoader(web_paths=[SOURCE_URL])
docs = loader.load()
print(docs)

"""## **Splitting document into chunk**"""

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = splitter.split_documents(docs)

"""## **Storing in Chromadb**"""

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(split_docs, embeddings)

"""## **Retriever**"""

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

"""## **State Definition**"""

class MessageClassifier(BaseModel):
  message_type: Literal["rag","logical"] = Field(
  ...,
  description="Classify if the message requires retrieval (rag) or a direct logical response."
  )

class State(TypedDict):
  messages: Annotated[list, add_messages]
  message_type: str | None

"""## **Classifier Node**"""

def classify_messages(state: State):
    last_message = state["messages"][-1]
    resp = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {
                "role": "system",
                "content": (
                    "Classify the user message as 'rag' if it likely benefits from external context "
                    "(e.g., asks about topics in the scraped document or requests factual info). "
                    "Otherwise classify as 'logical'. Reply with only 'rag' or 'logical'."
                )
            },
            {"role": "user", "content": last_message.content}
        ],
        temperature=0
    )
    label = resp.choices[0].message.content.strip().lower()
    if "rag" in label:
        return {"message_type": "rag"}
    elif "logical" in label:
        return {"message_type": "logical"}
    else:
        return {"message_type": "logical"}

"""## **RAG Agent**"""

def rag_agent(state: State):
  last_message = state["messages"][-1]
  retreived_docs = retriever.invoke(last_message.content)
  context = "\n\n".join([doc.page_content for doc in retrieved_docs[:3]]) or "No relevant context found."

  resp = client.chat.completions.create(
      model="llama-3.1-8b-instant",
      messages=[
          {
              "role":"system",
              "content":(
                  "You are a helpful assistant. Use the provided context to answer the user's question. "
                  "If context is insufficient, say so briefly and respond with best-effort clarity."
              )
          },
          {
              "role":"user",
              "content":f"Question: {last_message.content}\n\nContext:\n{context}"
          }
      ],
      temperature=0
  )

  reply = resp.choices[0].message.content.strip()
  return {"messages": [{"role": "assistant", "content": reply}]}

"""## **Logical Agent**"""

def logical_agent(state: State):
  last_message = state["messages"][-1]

  resp = client.chat.completions.create(
      model="llama-3.1-8b-instant",
      messages=[
          {
              "role":"system",
              "content":"You are a concise, factual assistant. Answer clearly and directly."

          },
          {
              "role":"user",
              "content":last_message.content
          }
      ],
      temperature=0
  )

  reply = resp.choices[0].message.content.strip()
  return {"messages": [{"role": "assistant", "content": reply}]}

"""## **Graph Builder with LangGraph**"""

graph_builder = StateGraph(State)

graph_builder.add_node("classifier", classify_messages)
graph_builder.add_node("rag_agent", rag_agent)
graph_builder.add_node("logical_agent", logical_agent)

graph_builder.add_edge(START, "classifier")

graph_builder.add_conditional_edges(
    "classifier",
    lambda state: state.get("message_type"),
    {
        "rag": "rag_agent",
        "logical": "logical_agent"
    }
)

graph_builder.add_edge("rag_agent", END)
graph_builder.add_edge("logical_agent", END)

graph = graph_builder.compile()

def run_chatbot():
    state = {"messages": [], "message_type": None}
    print("Type 'exit' to quit.")
    while True:
        user_input = input("Message: ")
        if user_input.strip().lower() == "exit":
            print("Bye")
            break

        # Add user message to state
        state["messages"].append({"role": "user", "content": user_input})

        # Invoke your LangGraph workflow (multi-agent pipeline)
        state = graph.invoke(state)

        # Find and print the assistant's reply
        assistant_output = None
        for msg in state["messages"]:
            role, content = normalize_message(msg)
            if role == "assistant":
                assistant_output = content

        if assistant_output:
            print(f"\nAssistant: {assistant_output}\n")
            print("-------------------------------------------------------------------------------------------------------")
        else:
            print("\nNo assistant output found.\n")

run_chatbot()